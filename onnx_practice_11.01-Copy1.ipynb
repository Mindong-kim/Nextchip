{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import shutil\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils import data\n",
    "from ptsemseg.models import get_model\n",
    "from ptsemseg.utils import convert_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    n_classes = 19\n",
    "\n",
    "    # Setup Model\n",
    "    model = get_model({\"arch\": \"hardnet\"}, n_classes)\n",
    "    state = convert_state_dict(torch.load(model_path, map_location=device)[\"model_state\"])\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    return device, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'weights/hardnet70_cityscapes_model.pkl'\n",
    "device, model = init_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = np.random.rand(512,1024,3)  # uint8 with RGB mode (h,w,c)\n",
    "img = dummy_input.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm\n",
    "value_scale = 255\n",
    "mean = [0.406, 0.456, 0.485]\n",
    "mean = [item * value_scale for item in mean]\n",
    "std = [0.225, 0.224, 0.229]\n",
    "std = [item * value_scale for item in std]\n",
    "img = (img - mean) / std\n",
    "\n",
    "# NHWC -> NCHW (n=batch size(한 배치 안의 이미지 개수),c,h,w)로 바꿔줌\n",
    "img = img.transpose(2, 0, 1)\n",
    "img = np.expand_dims(img, 0) #batch로 만들어줌\n",
    "img = torch.from_numpy(img).float()\n",
    "images = img.to(device) #gpu에 image올림\n",
    "outputs = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input.1 : Float(1:3, 3:1, 512:3072, 1024:3, requires_grad=0, device=cuda:0),\n",
      "      %finalConv.weight : Float(19:48, 48:1, 1:1, 1:1, requires_grad=1, device=cuda:0),\n",
      "      %finalConv.bias : Float(19:1, requires_grad=1, device=cuda:0),\n",
      "      %735 : Float(16:27, 3:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %736 : Float(16:1, requires_grad=0, device=cuda:0),\n",
      "      %738 : Float(24:144, 16:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %739 : Float(24:1, requires_grad=0, device=cuda:0),\n",
      "      %741 : Float(32:216, 24:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %742 : Float(32:1, requires_grad=0, device=cuda:0),\n",
      "      %744 : Float(48:288, 32:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %745 : Float(48:1, requires_grad=0, device=cuda:0),\n",
      "      %747 : Float(10:432, 48:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %748 : Float(10:1, requires_grad=0, device=cuda:0),\n",
      "      %750 : Float(18:522, 58:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %751 : Float(18:1, requires_grad=0, device=cuda:0),\n",
      "      %753 : Float(10:162, 18:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %754 : Float(10:1, requires_grad=0, device=cuda:0),\n",
      "      %756 : Float(28:684, 76:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %757 : Float(28:1, requires_grad=0, device=cuda:0),\n",
      "      %759 : Float(64:48, 48:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n",
      "      %760 : Float(64:1, requires_grad=0, device=cuda:0),\n",
      "      %762 : Float(16:576, 64:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %763 : Float(16:1, requires_grad=0, device=cuda:0),\n",
      "      %765 : Float(28:720, 80:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %766 : Float(28:1, requires_grad=0, device=cuda:0),\n",
      "      %768 : Float(16:252, 28:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %769 : Float(16:1, requires_grad=0, device=cuda:0),\n",
      "      %771 : Float(46:972, 108:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %772 : Float(46:1, requires_grad=0, device=cuda:0),\n",
      "      %774 : Float(96:78, 78:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n",
      "      %775 : Float(96:1, requires_grad=0, device=cuda:0),\n",
      "      %777 : Float(18:864, 96:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %778 : Float(18:1, requires_grad=0, device=cuda:0),\n",
      "      %780 : Float(30:1026, 114:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %781 : Float(30:1, requires_grad=0, device=cuda:0),\n",
      "      %783 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %784 : Float(18:1, requires_grad=0, device=cuda:0),\n",
      "      %786 : Float(52:1296, 144:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %787 : Float(52:1, requires_grad=0, device=cuda:0),\n",
      "      %789 : Float(18:468, 52:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %790 : Float(18:1, requires_grad=0, device=cuda:0),\n",
      "      %792 : Float(30:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %793 : Float(30:1, requires_grad=0, device=cuda:0),\n",
      "      %795 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %796 : Float(18:1, requires_grad=0, device=cuda:0),\n",
      "      %798 : Float(88:1764, 196:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %799 : Float(88:1, requires_grad=0, device=cuda:0),\n",
      "      %801 : Float(160:160, 160:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n",
      "      %802 : Float(160:1, requires_grad=0, device=cuda:0),\n",
      "      %804 : Float(24:1440, 160:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %805 : Float(24:1, requires_grad=0, device=cuda:0),\n",
      "      %807 : Float(40:1656, 184:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %808 : Float(40:1, requires_grad=0, device=cuda:0),\n",
      "      %810 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %811 : Float(24:1, requires_grad=0, device=cuda:0),\n",
      "      %813 : Float(70:2016, 224:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %814 : Float(70:1, requires_grad=0, device=cuda:0),\n",
      "      %816 : Float(24:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %817 : Float(24:1, requires_grad=0, device=cuda:0),\n",
      "      %819 : Float(40:846, 94:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %820 : Float(40:1, requires_grad=0, device=cuda:0),\n",
      "      %822 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %823 : Float(24:1, requires_grad=0, device=cuda:0),\n",
      "      %825 : Float(118:2646, 294:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %826 : Float(118:1, requires_grad=0, device=cuda:0),\n",
      "      %828 : Float(224:214, 214:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n",
      "      %829 : Float(224:1, requires_grad=0, device=cuda:0),\n",
      "      %831 : Float(32:2016, 224:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %832 : Float(32:1, requires_grad=0, device=cuda:0),\n",
      "      %834 : Float(54:2304, 256:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %835 : Float(54:1, requires_grad=0, device=cuda:0),\n",
      "      %837 : Float(32:486, 54:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %838 : Float(32:1, requires_grad=0, device=cuda:0),\n",
      "      %840 : Float(92:2790, 310:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %841 : Float(92:1, requires_grad=0, device=cuda:0),\n",
      "      %843 : Float(32:828, 92:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %844 : Float(32:1, requires_grad=0, device=cuda:0),\n",
      "      %846 : Float(54:1116, 124:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %847 : Float(54:1, requires_grad=0, device=cuda:0),\n",
      "      %849 : Float(32:486, 54:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %850 : Float(32:1, requires_grad=0, device=cuda:0),\n",
      "      %852 : Float(158:3618, 402:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %853 : Float(158:1, requires_grad=0, device=cuda:0),\n",
      "      %855 : Float(320:286, 286:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n",
      "      %856 : Float(320:1, requires_grad=0, device=cuda:0),\n",
      "      %858 : Float(267:534, 534:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n",
      "      %859 : Float(267:1, requires_grad=0, device=cuda:0),\n",
      "      %861 : Float(24:2403, 267:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %862 : Float(24:1, requires_grad=0, device=cuda:0),\n",
      "      %864 : Float(40:2619, 291:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %865 : Float(40:1, requires_grad=0, device=cuda:0),\n",
      "      %867 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %868 : Float(24:1, requires_grad=0, device=cuda:0),\n",
      "      %870 : Float(70:2979, 331:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %871 : Float(70:1, requires_grad=0, device=cuda:0),\n",
      "      %873 : Float(24:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %874 : Float(24:1, requires_grad=0, device=cuda:0),\n",
      "      %876 : Float(40:846, 94:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %877 : Float(40:1, requires_grad=0, device=cuda:0),\n",
      "      %879 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %880 : Float(24:1, requires_grad=0, device=cuda:0),\n",
      "      %882 : Float(118:3609, 401:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %883 : Float(118:1, requires_grad=0, device=cuda:0),\n",
      "      %885 : Float(187:374, 374:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n",
      "      %886 : Float(187:1, requires_grad=0, device=cuda:0),\n",
      "      %888 : Float(18:1683, 187:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %889 : Float(18:1, requires_grad=0, device=cuda:0),\n",
      "      %891 : Float(30:1845, 205:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %892 : Float(30:1, requires_grad=0, device=cuda:0),\n",
      "      %894 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %895 : Float(18:1, requires_grad=0, device=cuda:0),\n",
      "      %897 : Float(52:2115, 235:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %898 : Float(52:1, requires_grad=0, device=cuda:0),\n",
      "      %900 : Float(18:468, 52:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %901 : Float(18:1, requires_grad=0, device=cuda:0),\n",
      "      %903 : Float(30:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %904 : Float(30:1, requires_grad=0, device=cuda:0),\n",
      "      %906 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %907 : Float(18:1, requires_grad=0, device=cuda:0),\n",
      "      %909 : Float(88:2583, 287:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %910 : Float(88:1, requires_grad=0, device=cuda:0),\n",
      "      %912 : Float(119:238, 238:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n",
      "      %913 : Float(119:1, requires_grad=0, device=cuda:0),\n",
      "      %915 : Float(16:1071, 119:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %916 : Float(16:1, requires_grad=0, device=cuda:0),\n",
      "      %918 : Float(28:1215, 135:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %919 : Float(28:1, requires_grad=0, device=cuda:0),\n",
      "      %921 : Float(16:252, 28:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %922 : Float(16:1, requires_grad=0, device=cuda:0),\n",
      "      %924 : Float(46:1467, 163:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %925 : Float(46:1, requires_grad=0, device=cuda:0),\n",
      "      %927 : Float(63:126, 126:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n",
      "      %928 : Float(63:1, requires_grad=0, device=cuda:0),\n",
      "      %930 : Float(10:567, 63:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %931 : Float(10:1, requires_grad=0, device=cuda:0),\n",
      "      %933 : Float(18:657, 73:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %934 : Float(18:1, requires_grad=0, device=cuda:0),\n",
      "      %936 : Float(10:162, 18:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %937 : Float(10:1, requires_grad=0, device=cuda:0),\n",
      "      %939 : Float(28:819, 91:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n",
      "      %940 : Float(28:1, requires_grad=0, device=cuda:0)):\n",
      "  %417 : Tensor = onnx::Shape(%input.1)\n",
      "  %418 : Tensor = onnx::Constant[value={2}]()\n",
      "  %419 : Long(device=cpu) = onnx::Gather[axis=0](%417, %418) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:337:0\n",
      "  %420 : Tensor = onnx::Shape(%input.1)\n",
      "  %421 : Tensor = onnx::Constant[value={3}]()\n",
      "  %422 : Long(device=cpu) = onnx::Gather[axis=0](%420, %421) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:337:0\n",
      "  %734 : Float(1:2097152, 16:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%input.1, %735, %736)\n",
      "  %425 : Float(1:2097152, 16:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Relu(%734) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %737 : Float(1:3145728, 24:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%425, %738, %739)\n",
      "  %428 : Float(1:3145728, 24:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Relu(%737) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %740 : Float(1:1048576, 32:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%428, %741, %742)\n",
      "  %431 : Float(1:1048576, 32:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%740) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %743 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%431, %744, %745)\n",
      "  %434 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%743) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %746 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%434, %747, %748)\n",
      "  %437 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%746) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %438 : Float(1:1900544, 58:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%437, %434) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %749 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%438, %750, %751)\n",
      "  %441 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%749) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %752 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%441, %753, %754)\n",
      "  %444 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%752) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %445 : Float(1:2490368, 76:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%444, %441, %434) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %755 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%445, %756, %757)\n",
      "  %448 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%755) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %449 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%437, %444, %448) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n",
      "  %758 : Float(1:2097152, 64:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%449, %759, %760)\n",
      "  %452 : Float(1:2097152, 64:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%758) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %453 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%452)\n",
      "  %454 : Float(1:524288, 64:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%453) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n",
      "  %761 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%454, %762, %763)\n",
      "  %457 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%761) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %458 : Float(1:655360, 80:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%457, %454) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %764 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%458, %765, %766)\n",
      "  %461 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%764) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %767 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%461, %768, %769)\n",
      "  %464 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%767) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %465 : Float(1:884736, 108:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%464, %461, %454) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %770 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%465, %771, %772)\n",
      "  %468 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%770) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %469 : Float(1:638976, 78:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%457, %464, %468) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n",
      "  %773 : Float(1:786432, 96:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%469, %774, %775)\n",
      "  %472 : Float(1:786432, 96:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%773) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %473 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%472)\n",
      "  %474 : Float(1:196608, 96:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%473) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n",
      "  %776 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%474, %777, %778)\n",
      "  %477 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%776) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %478 : Float(1:233472, 114:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%477, %474) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %779 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%478, %780, %781)\n",
      "  %481 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%779) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %782 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%481, %783, %784)\n",
      "  %484 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%782) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %485 : Float(1:294912, 144:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%484, %481, %474) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %785 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%485, %786, %787)\n",
      "  %488 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%785) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %788 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%488, %789, %790)\n",
      "  %491 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%788) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %492 : Float(1:143360, 70:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%491, %488) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %791 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%492, %792, %793)\n",
      "  %495 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%791) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %794 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%495, %795, %796)\n",
      "  %498 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%794) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %499 : Float(1:401408, 196:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%498, %495, %488, %474) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %797 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%499, %798, %799)\n",
      "  %502 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%797) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %503 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%477, %484, %491, %498, %502) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n",
      "  %800 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%503, %801, %802)\n",
      "  %506 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%800) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %507 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%506)\n",
      "  %508 : Float(1:81920, 160:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%507) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n",
      "  %803 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%508, %804, %805)\n",
      "  %511 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%803) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %512 : Float(1:94208, 184:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%511, %508) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %806 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%512, %807, %808)\n",
      "  %515 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%806) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %809 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%515, %810, %811)\n",
      "  %518 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%809) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %519 : Float(1:114688, 224:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%518, %515, %508) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %812 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%519, %813, %814)\n",
      "  %522 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%812) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %815 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%522, %816, %817)\n",
      "  %525 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%815) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %526 : Float(1:48128, 94:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%525, %522) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %818 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%526, %819, %820)\n",
      "  %529 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%818) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %821 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%529, %822, %823)\n",
      "  %532 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%821) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %533 : Float(1:150528, 294:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%532, %529, %522, %508) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %824 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%533, %825, %826)\n",
      "  %536 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%824) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %537 : Float(1:109568, 214:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%511, %518, %525, %532, %536) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n",
      "  %827 : Float(1:114688, 224:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%537, %828, %829)\n",
      "  %540 : Float(1:114688, 224:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%827) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %541 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%540)\n",
      "  %542 : Float(1:28672, 224:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%541) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n",
      "  %830 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%542, %831, %832)\n",
      "  %545 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%830) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %546 : Float(1:32768, 256:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%545, %542) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %833 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%546, %834, %835)\n",
      "  %549 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%833) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %836 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%549, %837, %838)\n",
      "  %552 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%836) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %553 : Float(1:39680, 310:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%552, %549, %542) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %839 : Float(1:11776, 92:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%553, %840, %841)\n",
      "  %556 : Float(1:11776, 92:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%839) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %842 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%556, %843, %844)\n",
      "  %559 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%842) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %560 : Float(1:15872, 124:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%559, %556) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %845 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%560, %846, %847)\n",
      "  %563 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%845) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %848 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%563, %849, %850)\n",
      "  %566 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%848) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %567 : Float(1:51456, 402:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%566, %563, %556, %542) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %851 : Float(1:20224, 158:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%567, %852, %853)\n",
      "  %570 : Float(1:20224, 158:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%851) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %571 : Float(1:36608, 286:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%545, %552, %559, %566, %570) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n",
      "  %854 : Float(1:40960, 320:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%571, %855, %856)\n",
      "  %574 : Float(1:40960, 320:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%854) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %575 : Tensor = onnx::Shape(%537)\n",
      "  %576 : Tensor = onnx::Constant[value={2}]()\n",
      "  %577 : Long(device=cpu) = onnx::Gather[axis=0](%575, %576) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n",
      "  %578 : Tensor = onnx::Shape(%537)\n",
      "  %579 : Tensor = onnx::Constant[value={3}]()\n",
      "  %580 : Long(device=cpu) = onnx::Gather[axis=0](%578, %579) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n",
      "  %581 : Tensor = onnx::Unsqueeze[axes=[0]](%577)\n",
      "  %582 : Tensor = onnx::Unsqueeze[axes=[0]](%580)\n",
      "  %583 : Tensor = onnx::Concat[axis=0](%581, %582)\n",
      "  %584 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n",
      "  %585 : None = prim::Constant()\n",
      "  %586 : Float(1:163840, 320:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%574, %583, %584, %585) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n",
      "  %587 : Float(1:273408, 534:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%586, %537) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n",
      "  %857 : Float(1:136704, 267:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%587, %858, %859)\n",
      "  %590 : Float(1:136704, 267:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%857) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %860 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%590, %861, %862)\n",
      "  %593 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%860) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %594 : Float(1:148992, 291:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%593, %590) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %863 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%594, %864, %865)\n",
      "  %597 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%863) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %866 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%597, %867, %868)\n",
      "  %600 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%866) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %601 : Float(1:169472, 331:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%600, %597, %590) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %869 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%601, %870, %871)\n",
      "  %604 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%869) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %872 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%604, %873, %874)\n",
      "  %607 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%872) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %608 : Float(1:48128, 94:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%607, %604) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %875 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%608, %876, %877)\n",
      "  %611 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%875) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %878 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%611, %879, %880)\n",
      "  %614 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%878) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %615 : Float(1:205312, 401:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%614, %611, %604, %590) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %881 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%615, %882, %883)\n",
      "  %618 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%881) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %619 : Float(1:109568, 214:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%593, %600, %607, %614, %618) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n",
      "  %620 : Tensor = onnx::Shape(%503)\n",
      "  %621 : Tensor = onnx::Constant[value={2}]()\n",
      "  %622 : Long(device=cpu) = onnx::Gather[axis=0](%620, %621) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n",
      "  %623 : Tensor = onnx::Shape(%503)\n",
      "  %624 : Tensor = onnx::Constant[value={3}]()\n",
      "  %625 : Long(device=cpu) = onnx::Gather[axis=0](%623, %624) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n",
      "  %626 : Tensor = onnx::Unsqueeze[axes=[0]](%622)\n",
      "  %627 : Tensor = onnx::Unsqueeze[axes=[0]](%625)\n",
      "  %628 : Tensor = onnx::Concat[axis=0](%626, %627)\n",
      "  %629 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n",
      "  %630 : None = prim::Constant()\n",
      "  %631 : Float(1:438272, 214:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%619, %628, %629, %630) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n",
      "  %632 : Float(1:765952, 374:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%631, %503) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n",
      "  %884 : Float(1:382976, 187:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%632, %885, %886)\n",
      "  %635 : Float(1:382976, 187:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%884) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %887 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%635, %888, %889)\n",
      "  %638 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%887) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %639 : Float(1:419840, 205:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%638, %635) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %890 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%639, %891, %892)\n",
      "  %642 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%890) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %893 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%642, %894, %895)\n",
      "  %645 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%893) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %646 : Float(1:481280, 235:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%645, %642, %635) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %896 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%646, %897, %898)\n",
      "  %649 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%896) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %899 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%649, %900, %901)\n",
      "  %652 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%899) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %653 : Float(1:143360, 70:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%652, %649) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %902 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%653, %903, %904)\n",
      "  %656 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%902) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %905 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%656, %906, %907)\n",
      "  %659 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%905) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %660 : Float(1:587776, 287:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%659, %656, %649, %635) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %908 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%660, %909, %910)\n",
      "  %663 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%908) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %664 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%638, %645, %652, %659, %663) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n",
      "  %665 : Tensor = onnx::Shape(%469)\n",
      "  %666 : Tensor = onnx::Constant[value={2}]()\n",
      "  %667 : Long(device=cpu) = onnx::Gather[axis=0](%665, %666) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n",
      "  %668 : Tensor = onnx::Shape(%469)\n",
      "  %669 : Tensor = onnx::Constant[value={3}]()\n",
      "  %670 : Long(device=cpu) = onnx::Gather[axis=0](%668, %669) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n",
      "  %671 : Tensor = onnx::Unsqueeze[axes=[0]](%667)\n",
      "  %672 : Tensor = onnx::Unsqueeze[axes=[0]](%670)\n",
      "  %673 : Tensor = onnx::Concat[axis=0](%671, %672)\n",
      "  %674 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n",
      "  %675 : None = prim::Constant()\n",
      "  %676 : Float(1:1310720, 160:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%664, %673, %674, %675) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n",
      "  %677 : Float(1:1949696, 238:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%676, %469) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n",
      "  %911 : Float(1:974848, 119:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%677, %912, %913)\n",
      "  %680 : Float(1:974848, 119:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%911) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %914 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%680, %915, %916)\n",
      "  %683 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%914) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %684 : Float(1:1105920, 135:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%683, %680) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %917 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%684, %918, %919)\n",
      "  %687 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%917) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %920 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%687, %921, %922)\n",
      "  %690 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%920) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %691 : Float(1:1335296, 163:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%690, %687, %680) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %923 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%691, %924, %925)\n",
      "  %694 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%923) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %695 : Float(1:638976, 78:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%683, %690, %694) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n",
      "  %696 : Tensor = onnx::Shape(%449)\n",
      "  %697 : Tensor = onnx::Constant[value={2}]()\n",
      "  %698 : Long(device=cpu) = onnx::Gather[axis=0](%696, %697) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n",
      "  %699 : Tensor = onnx::Shape(%449)\n",
      "  %700 : Tensor = onnx::Constant[value={3}]()\n",
      "  %701 : Long(device=cpu) = onnx::Gather[axis=0](%699, %700) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n",
      "  %702 : Tensor = onnx::Unsqueeze[axes=[0]](%698)\n",
      "  %703 : Tensor = onnx::Unsqueeze[axes=[0]](%701)\n",
      "  %704 : Tensor = onnx::Concat[axis=0](%702, %703)\n",
      "  %705 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n",
      "  %706 : None = prim::Constant()\n",
      "  %707 : Float(1:2555904, 78:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%695, %704, %705, %706) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n",
      "  %708 : Float(1:4128768, 126:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%707, %449) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n",
      "  %926 : Float(1:2064384, 63:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%708, %927, %928)\n",
      "  %711 : Float(1:2064384, 63:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%926) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %929 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%711, %930, %931)\n",
      "  %714 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%929) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %715 : Float(1:2392064, 73:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%714, %711) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %932 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%715, %933, %934)\n",
      "  %718 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%932) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %935 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%718, %936, %937)\n",
      "  %721 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%935) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %722 : Float(1:2981888, 91:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%721, %718, %711) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n",
      "  %938 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%722, %939, %940)\n",
      "  %725 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%938) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n",
      "  %726 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%714, %721, %725) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n",
      "  %727 : Float(1:622592, 19:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%726, %finalConv.weight, %finalConv.bias) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/conv.py:420:0\n",
      "  %728 : Tensor = onnx::Unsqueeze[axes=[0]](%419)\n",
      "  %729 : Tensor = onnx::Unsqueeze[axes=[0]](%422)\n",
      "  %730 : Tensor = onnx::Concat[axis=0](%728, %729)\n",
      "  %731 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n",
      "  %732 : None = prim::Constant()\n",
      "  %733 : Float(1:9961472, 19:524288, 512:1024, 1024:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%727, %730, %731, %732) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n",
      "  return (%733)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ONNX export failed: Couldn't export operator aten::upsample_bilinear2d\n\nDefined at:\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py(3151): interpolate\n/home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py(247): forward\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/module.py(725): _call_impl\n/home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py(348): forward\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/module.py(725): _call_impl\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/jit/_trace.py(116): wrapper\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/jit/_trace.py(130): forward\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/module.py(727): _call_impl\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/jit/_trace.py(1148): _get_trace_graph\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py(342): _trace_and_get_graph_from_model\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py(379): _create_jit_graph\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py(411): _model_to_graph\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py(639): _export\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py(91): export\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/__init__.py(230): export\n<ipython-input-9-5d8c14845a8e>(3): <module>\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3343): run_code\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3263): run_ast_nodes\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3072): run_cell_async\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/async_helpers.py(68): _pseudo_sync_runner\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2895): _run_cell\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2867): run_cell\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/zmqshell.py(536): run_cell\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/ipkernel.py(306): do_execute\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/gen.py(209): wrapper\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/kernelbase.py(545): execute_request\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/gen.py(209): wrapper\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/kernelbase.py(268): dispatch_shell\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/gen.py(209): wrapper\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/kernelbase.py(365): process_one\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/gen.py(748): run\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/gen.py(787): inner\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/ioloop.py(743): _run_callback\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/ioloop.py(690): <lambda>\n/usr/lib/python3.6/asyncio/events.py(145): _run\n/usr/lib/python3.6/asyncio/base_events.py(1451): _run_once\n/usr/lib/python3.6/asyncio/base_events.py(438): run_forever\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/platform/asyncio.py(149): start\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/kernelapp.py(612): start\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/traitlets/config/application.py(664): launch_instance\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel_launcher.py(16): <module>\n/usr/lib/python3.6/runpy.py(85): _run_code\n/usr/lib/python3.6/runpy.py(193): _run_module_as_main\n\n\nGraph we tried to export:\ngraph(%input.1 : Float(1:3, 3:1, 512:3072, 1024:3, requires_grad=0, device=cuda:0),\n      %finalConv.weight : Float(19:48, 48:1, 1:1, 1:1, requires_grad=1, device=cuda:0),\n      %finalConv.bias : Float(19:1, requires_grad=1, device=cuda:0),\n      %735 : Float(16:27, 3:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %736 : Float(16:1, requires_grad=0, device=cuda:0),\n      %738 : Float(24:144, 16:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %739 : Float(24:1, requires_grad=0, device=cuda:0),\n      %741 : Float(32:216, 24:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %742 : Float(32:1, requires_grad=0, device=cuda:0),\n      %744 : Float(48:288, 32:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %745 : Float(48:1, requires_grad=0, device=cuda:0),\n      %747 : Float(10:432, 48:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %748 : Float(10:1, requires_grad=0, device=cuda:0),\n      %750 : Float(18:522, 58:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %751 : Float(18:1, requires_grad=0, device=cuda:0),\n      %753 : Float(10:162, 18:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %754 : Float(10:1, requires_grad=0, device=cuda:0),\n      %756 : Float(28:684, 76:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %757 : Float(28:1, requires_grad=0, device=cuda:0),\n      %759 : Float(64:48, 48:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %760 : Float(64:1, requires_grad=0, device=cuda:0),\n      %762 : Float(16:576, 64:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %763 : Float(16:1, requires_grad=0, device=cuda:0),\n      %765 : Float(28:720, 80:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %766 : Float(28:1, requires_grad=0, device=cuda:0),\n      %768 : Float(16:252, 28:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %769 : Float(16:1, requires_grad=0, device=cuda:0),\n      %771 : Float(46:972, 108:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %772 : Float(46:1, requires_grad=0, device=cuda:0),\n      %774 : Float(96:78, 78:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %775 : Float(96:1, requires_grad=0, device=cuda:0),\n      %777 : Float(18:864, 96:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %778 : Float(18:1, requires_grad=0, device=cuda:0),\n      %780 : Float(30:1026, 114:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %781 : Float(30:1, requires_grad=0, device=cuda:0),\n      %783 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %784 : Float(18:1, requires_grad=0, device=cuda:0),\n      %786 : Float(52:1296, 144:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %787 : Float(52:1, requires_grad=0, device=cuda:0),\n      %789 : Float(18:468, 52:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %790 : Float(18:1, requires_grad=0, device=cuda:0),\n      %792 : Float(30:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %793 : Float(30:1, requires_grad=0, device=cuda:0),\n      %795 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %796 : Float(18:1, requires_grad=0, device=cuda:0),\n      %798 : Float(88:1764, 196:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %799 : Float(88:1, requires_grad=0, device=cuda:0),\n      %801 : Float(160:160, 160:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %802 : Float(160:1, requires_grad=0, device=cuda:0),\n      %804 : Float(24:1440, 160:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %805 : Float(24:1, requires_grad=0, device=cuda:0),\n      %807 : Float(40:1656, 184:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %808 : Float(40:1, requires_grad=0, device=cuda:0),\n      %810 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %811 : Float(24:1, requires_grad=0, device=cuda:0),\n      %813 : Float(70:2016, 224:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %814 : Float(70:1, requires_grad=0, device=cuda:0),\n      %816 : Float(24:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %817 : Float(24:1, requires_grad=0, device=cuda:0),\n      %819 : Float(40:846, 94:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %820 : Float(40:1, requires_grad=0, device=cuda:0),\n      %822 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %823 : Float(24:1, requires_grad=0, device=cuda:0),\n      %825 : Float(118:2646, 294:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %826 : Float(118:1, requires_grad=0, device=cuda:0),\n      %828 : Float(224:214, 214:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %829 : Float(224:1, requires_grad=0, device=cuda:0),\n      %831 : Float(32:2016, 224:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %832 : Float(32:1, requires_grad=0, device=cuda:0),\n      %834 : Float(54:2304, 256:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %835 : Float(54:1, requires_grad=0, device=cuda:0),\n      %837 : Float(32:486, 54:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %838 : Float(32:1, requires_grad=0, device=cuda:0),\n      %840 : Float(92:2790, 310:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %841 : Float(92:1, requires_grad=0, device=cuda:0),\n      %843 : Float(32:828, 92:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %844 : Float(32:1, requires_grad=0, device=cuda:0),\n      %846 : Float(54:1116, 124:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %847 : Float(54:1, requires_grad=0, device=cuda:0),\n      %849 : Float(32:486, 54:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %850 : Float(32:1, requires_grad=0, device=cuda:0),\n      %852 : Float(158:3618, 402:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %853 : Float(158:1, requires_grad=0, device=cuda:0),\n      %855 : Float(320:286, 286:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %856 : Float(320:1, requires_grad=0, device=cuda:0),\n      %858 : Float(267:534, 534:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %859 : Float(267:1, requires_grad=0, device=cuda:0),\n      %861 : Float(24:2403, 267:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %862 : Float(24:1, requires_grad=0, device=cuda:0),\n      %864 : Float(40:2619, 291:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %865 : Float(40:1, requires_grad=0, device=cuda:0),\n      %867 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %868 : Float(24:1, requires_grad=0, device=cuda:0),\n      %870 : Float(70:2979, 331:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %871 : Float(70:1, requires_grad=0, device=cuda:0),\n      %873 : Float(24:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %874 : Float(24:1, requires_grad=0, device=cuda:0),\n      %876 : Float(40:846, 94:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %877 : Float(40:1, requires_grad=0, device=cuda:0),\n      %879 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %880 : Float(24:1, requires_grad=0, device=cuda:0),\n      %882 : Float(118:3609, 401:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %883 : Float(118:1, requires_grad=0, device=cuda:0),\n      %885 : Float(187:374, 374:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %886 : Float(187:1, requires_grad=0, device=cuda:0),\n      %888 : Float(18:1683, 187:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %889 : Float(18:1, requires_grad=0, device=cuda:0),\n      %891 : Float(30:1845, 205:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %892 : Float(30:1, requires_grad=0, device=cuda:0),\n      %894 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %895 : Float(18:1, requires_grad=0, device=cuda:0),\n      %897 : Float(52:2115, 235:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %898 : Float(52:1, requires_grad=0, device=cuda:0),\n      %900 : Float(18:468, 52:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %901 : Float(18:1, requires_grad=0, device=cuda:0),\n      %903 : Float(30:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %904 : Float(30:1, requires_grad=0, device=cuda:0),\n      %906 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %907 : Float(18:1, requires_grad=0, device=cuda:0),\n      %909 : Float(88:2583, 287:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %910 : Float(88:1, requires_grad=0, device=cuda:0),\n      %912 : Float(119:238, 238:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %913 : Float(119:1, requires_grad=0, device=cuda:0),\n      %915 : Float(16:1071, 119:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %916 : Float(16:1, requires_grad=0, device=cuda:0),\n      %918 : Float(28:1215, 135:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %919 : Float(28:1, requires_grad=0, device=cuda:0),\n      %921 : Float(16:252, 28:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %922 : Float(16:1, requires_grad=0, device=cuda:0),\n      %924 : Float(46:1467, 163:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %925 : Float(46:1, requires_grad=0, device=cuda:0),\n      %927 : Float(63:126, 126:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %928 : Float(63:1, requires_grad=0, device=cuda:0),\n      %930 : Float(10:567, 63:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %931 : Float(10:1, requires_grad=0, device=cuda:0),\n      %933 : Float(18:657, 73:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %934 : Float(18:1, requires_grad=0, device=cuda:0),\n      %936 : Float(10:162, 18:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %937 : Float(10:1, requires_grad=0, device=cuda:0),\n      %939 : Float(28:819, 91:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %940 : Float(28:1, requires_grad=0, device=cuda:0)):\n  %417 : Tensor = onnx::Shape(%input.1)\n  %418 : Tensor = onnx::Constant[value={2}]()\n  %419 : Long(device=cpu) = onnx::Gather[axis=0](%417, %418) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:337:0\n  %420 : Tensor = onnx::Shape(%input.1)\n  %421 : Tensor = onnx::Constant[value={3}]()\n  %422 : Long(device=cpu) = onnx::Gather[axis=0](%420, %421) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:337:0\n  %734 : Float(1:2097152, 16:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%input.1, %735, %736)\n  %425 : Float(1:2097152, 16:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Relu(%734) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %737 : Float(1:3145728, 24:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%425, %738, %739)\n  %428 : Float(1:3145728, 24:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Relu(%737) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %740 : Float(1:1048576, 32:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%428, %741, %742)\n  %431 : Float(1:1048576, 32:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%740) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %743 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%431, %744, %745)\n  %434 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%743) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %746 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%434, %747, %748)\n  %437 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%746) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %438 : Float(1:1900544, 58:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%437, %434) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %749 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%438, %750, %751)\n  %441 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%749) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %752 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%441, %753, %754)\n  %444 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%752) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %445 : Float(1:2490368, 76:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%444, %441, %434) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %755 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%445, %756, %757)\n  %448 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%755) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %449 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%437, %444, %448) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %758 : Float(1:2097152, 64:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%449, %759, %760)\n  %452 : Float(1:2097152, 64:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%758) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %453 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%452)\n  %454 : Float(1:524288, 64:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%453) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n  %761 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%454, %762, %763)\n  %457 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%761) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %458 : Float(1:655360, 80:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%457, %454) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %764 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%458, %765, %766)\n  %461 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%764) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %767 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%461, %768, %769)\n  %464 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%767) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %465 : Float(1:884736, 108:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%464, %461, %454) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %770 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%465, %771, %772)\n  %468 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%770) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %469 : Float(1:638976, 78:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%457, %464, %468) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %773 : Float(1:786432, 96:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%469, %774, %775)\n  %472 : Float(1:786432, 96:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%773) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %473 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%472)\n  %474 : Float(1:196608, 96:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%473) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n  %776 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%474, %777, %778)\n  %477 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%776) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %478 : Float(1:233472, 114:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%477, %474) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %779 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%478, %780, %781)\n  %481 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%779) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %782 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%481, %783, %784)\n  %484 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%782) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %485 : Float(1:294912, 144:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%484, %481, %474) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %785 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%485, %786, %787)\n  %488 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%785) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %788 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%488, %789, %790)\n  %491 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%788) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %492 : Float(1:143360, 70:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%491, %488) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %791 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%492, %792, %793)\n  %495 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%791) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %794 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%495, %795, %796)\n  %498 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%794) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %499 : Float(1:401408, 196:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%498, %495, %488, %474) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %797 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%499, %798, %799)\n  %502 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%797) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %503 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%477, %484, %491, %498, %502) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %800 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%503, %801, %802)\n  %506 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%800) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %507 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%506)\n  %508 : Float(1:81920, 160:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%507) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n  %803 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%508, %804, %805)\n  %511 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%803) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %512 : Float(1:94208, 184:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%511, %508) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %806 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%512, %807, %808)\n  %515 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%806) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %809 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%515, %810, %811)\n  %518 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%809) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %519 : Float(1:114688, 224:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%518, %515, %508) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %812 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%519, %813, %814)\n  %522 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%812) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %815 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%522, %816, %817)\n  %525 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%815) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %526 : Float(1:48128, 94:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%525, %522) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %818 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%526, %819, %820)\n  %529 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%818) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %821 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%529, %822, %823)\n  %532 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%821) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %533 : Float(1:150528, 294:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%532, %529, %522, %508) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %824 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%533, %825, %826)\n  %536 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%824) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %537 : Float(1:109568, 214:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%511, %518, %525, %532, %536) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %827 : Float(1:114688, 224:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%537, %828, %829)\n  %540 : Float(1:114688, 224:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%827) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %541 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%540)\n  %542 : Float(1:28672, 224:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%541) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n  %830 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%542, %831, %832)\n  %545 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%830) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %546 : Float(1:32768, 256:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%545, %542) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %833 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%546, %834, %835)\n  %549 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%833) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %836 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%549, %837, %838)\n  %552 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%836) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %553 : Float(1:39680, 310:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%552, %549, %542) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %839 : Float(1:11776, 92:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%553, %840, %841)\n  %556 : Float(1:11776, 92:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%839) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %842 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%556, %843, %844)\n  %559 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%842) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %560 : Float(1:15872, 124:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%559, %556) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %845 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%560, %846, %847)\n  %563 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%845) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %848 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%563, %849, %850)\n  %566 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%848) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %567 : Float(1:51456, 402:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%566, %563, %556, %542) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %851 : Float(1:20224, 158:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%567, %852, %853)\n  %570 : Float(1:20224, 158:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%851) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %571 : Float(1:36608, 286:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%545, %552, %559, %566, %570) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %854 : Float(1:40960, 320:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%571, %855, %856)\n  %574 : Float(1:40960, 320:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%854) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %575 : Tensor = onnx::Shape(%537)\n  %576 : Tensor = onnx::Constant[value={2}]()\n  %577 : Long(device=cpu) = onnx::Gather[axis=0](%575, %576) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %578 : Tensor = onnx::Shape(%537)\n  %579 : Tensor = onnx::Constant[value={3}]()\n  %580 : Long(device=cpu) = onnx::Gather[axis=0](%578, %579) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %581 : Tensor = onnx::Unsqueeze[axes=[0]](%577)\n  %582 : Tensor = onnx::Unsqueeze[axes=[0]](%580)\n  %583 : Tensor = onnx::Concat[axis=0](%581, %582)\n  %584 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %585 : None = prim::Constant()\n  %586 : Float(1:163840, 320:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%574, %583, %584, %585) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n  %587 : Float(1:273408, 534:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%586, %537) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n  %857 : Float(1:136704, 267:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%587, %858, %859)\n  %590 : Float(1:136704, 267:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%857) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %860 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%590, %861, %862)\n  %593 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%860) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %594 : Float(1:148992, 291:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%593, %590) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %863 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%594, %864, %865)\n  %597 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%863) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %866 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%597, %867, %868)\n  %600 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%866) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %601 : Float(1:169472, 331:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%600, %597, %590) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %869 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%601, %870, %871)\n  %604 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%869) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %872 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%604, %873, %874)\n  %607 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%872) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %608 : Float(1:48128, 94:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%607, %604) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %875 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%608, %876, %877)\n  %611 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%875) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %878 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%611, %879, %880)\n  %614 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%878) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %615 : Float(1:205312, 401:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%614, %611, %604, %590) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %881 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%615, %882, %883)\n  %618 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%881) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %619 : Float(1:109568, 214:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%593, %600, %607, %614, %618) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %620 : Tensor = onnx::Shape(%503)\n  %621 : Tensor = onnx::Constant[value={2}]()\n  %622 : Long(device=cpu) = onnx::Gather[axis=0](%620, %621) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %623 : Tensor = onnx::Shape(%503)\n  %624 : Tensor = onnx::Constant[value={3}]()\n  %625 : Long(device=cpu) = onnx::Gather[axis=0](%623, %624) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %626 : Tensor = onnx::Unsqueeze[axes=[0]](%622)\n  %627 : Tensor = onnx::Unsqueeze[axes=[0]](%625)\n  %628 : Tensor = onnx::Concat[axis=0](%626, %627)\n  %629 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %630 : None = prim::Constant()\n  %631 : Float(1:438272, 214:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%619, %628, %629, %630) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n  %632 : Float(1:765952, 374:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%631, %503) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n  %884 : Float(1:382976, 187:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%632, %885, %886)\n  %635 : Float(1:382976, 187:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%884) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %887 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%635, %888, %889)\n  %638 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%887) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %639 : Float(1:419840, 205:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%638, %635) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %890 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%639, %891, %892)\n  %642 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%890) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %893 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%642, %894, %895)\n  %645 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%893) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %646 : Float(1:481280, 235:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%645, %642, %635) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %896 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%646, %897, %898)\n  %649 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%896) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %899 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%649, %900, %901)\n  %652 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%899) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %653 : Float(1:143360, 70:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%652, %649) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %902 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%653, %903, %904)\n  %656 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%902) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %905 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%656, %906, %907)\n  %659 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%905) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %660 : Float(1:587776, 287:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%659, %656, %649, %635) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %908 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%660, %909, %910)\n  %663 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%908) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %664 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%638, %645, %652, %659, %663) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %665 : Tensor = onnx::Shape(%469)\n  %666 : Tensor = onnx::Constant[value={2}]()\n  %667 : Long(device=cpu) = onnx::Gather[axis=0](%665, %666) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %668 : Tensor = onnx::Shape(%469)\n  %669 : Tensor = onnx::Constant[value={3}]()\n  %670 : Long(device=cpu) = onnx::Gather[axis=0](%668, %669) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %671 : Tensor = onnx::Unsqueeze[axes=[0]](%667)\n  %672 : Tensor = onnx::Unsqueeze[axes=[0]](%670)\n  %673 : Tensor = onnx::Concat[axis=0](%671, %672)\n  %674 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %675 : None = prim::Constant()\n  %676 : Float(1:1310720, 160:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%664, %673, %674, %675) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n  %677 : Float(1:1949696, 238:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%676, %469) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n  %911 : Float(1:974848, 119:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%677, %912, %913)\n  %680 : Float(1:974848, 119:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%911) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %914 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%680, %915, %916)\n  %683 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%914) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %684 : Float(1:1105920, 135:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%683, %680) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %917 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%684, %918, %919)\n  %687 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%917) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %920 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%687, %921, %922)\n  %690 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%920) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %691 : Float(1:1335296, 163:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%690, %687, %680) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %923 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%691, %924, %925)\n  %694 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%923) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %695 : Float(1:638976, 78:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%683, %690, %694) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %696 : Tensor = onnx::Shape(%449)\n  %697 : Tensor = onnx::Constant[value={2}]()\n  %698 : Long(device=cpu) = onnx::Gather[axis=0](%696, %697) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %699 : Tensor = onnx::Shape(%449)\n  %700 : Tensor = onnx::Constant[value={3}]()\n  %701 : Long(device=cpu) = onnx::Gather[axis=0](%699, %700) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %702 : Tensor = onnx::Unsqueeze[axes=[0]](%698)\n  %703 : Tensor = onnx::Unsqueeze[axes=[0]](%701)\n  %704 : Tensor = onnx::Concat[axis=0](%702, %703)\n  %705 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %706 : None = prim::Constant()\n  %707 : Float(1:2555904, 78:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%695, %704, %705, %706) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n  %708 : Float(1:4128768, 126:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%707, %449) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n  %926 : Float(1:2064384, 63:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%708, %927, %928)\n  %711 : Float(1:2064384, 63:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%926) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %929 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%711, %930, %931)\n  %714 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%929) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %715 : Float(1:2392064, 73:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%714, %711) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %932 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%715, %933, %934)\n  %718 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%932) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %935 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%718, %936, %937)\n  %721 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%935) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %722 : Float(1:2981888, 91:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%721, %718, %711) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %938 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%722, %939, %940)\n  %725 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%938) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %726 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%714, %721, %725) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %727 : Float(1:622592, 19:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%726, %finalConv.weight, %finalConv.bias) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/conv.py:420:0\n  %728 : Tensor = onnx::Unsqueeze[axes=[0]](%419)\n  %729 : Tensor = onnx::Unsqueeze[axes=[0]](%422)\n  %730 : Tensor = onnx::Concat[axis=0](%728, %729)\n  %731 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %732 : None = prim::Constant()\n  %733 : Float(1:9961472, 19:524288, 512:1024, 1024:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%727, %730, %731, %732) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n  return (%733)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5d8c14845a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Export the trained model to ONNX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"FCHarDNet.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopset_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#torch.onnx.export(model,images,\"FCHarDNet1.onnx\",opset_version=9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m    228\u001b[0m                         \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0mstrip_doc_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                         custom_opsets, enable_onnx_checker, use_external_data_format)\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_onnx_checker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_onnx_checker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             use_external_data_format=use_external_data_format)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format, onnx_shape_inference, use_new_jit_passes)\u001b[0m\n\u001b[1;32m    648\u001b[0m                     \u001b[0mparams_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefer_weight_export\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                     \u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_doc_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_keep_init_as_ip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                     val_add_node_names, val_use_external_data_format, model_file_location)\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 proto, export_map = graph._export_onnx(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ONNX export failed: Couldn't export operator aten::upsample_bilinear2d\n\nDefined at:\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py(3151): interpolate\n/home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py(247): forward\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/module.py(725): _call_impl\n/home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py(348): forward\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/module.py(725): _call_impl\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/jit/_trace.py(116): wrapper\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/jit/_trace.py(130): forward\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/module.py(727): _call_impl\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/jit/_trace.py(1148): _get_trace_graph\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py(342): _trace_and_get_graph_from_model\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py(379): _create_jit_graph\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py(411): _model_to_graph\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py(639): _export\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/utils.py(91): export\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/onnx/__init__.py(230): export\n<ipython-input-9-5d8c14845a8e>(3): <module>\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3343): run_code\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3263): run_ast_nodes\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3072): run_cell_async\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/async_helpers.py(68): _pseudo_sync_runner\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2895): _run_cell\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2867): run_cell\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/zmqshell.py(536): run_cell\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/ipkernel.py(306): do_execute\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/gen.py(209): wrapper\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/kernelbase.py(545): execute_request\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/gen.py(209): wrapper\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/kernelbase.py(268): dispatch_shell\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/gen.py(209): wrapper\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/kernelbase.py(365): process_one\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/gen.py(748): run\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/gen.py(787): inner\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/ioloop.py(743): _run_callback\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/ioloop.py(690): <lambda>\n/usr/lib/python3.6/asyncio/events.py(145): _run\n/usr/lib/python3.6/asyncio/base_events.py(1451): _run_once\n/usr/lib/python3.6/asyncio/base_events.py(438): run_forever\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/tornado/platform/asyncio.py(149): start\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel/kernelapp.py(612): start\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/traitlets/config/application.py(664): launch_instance\n/home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/ipykernel_launcher.py(16): <module>\n/usr/lib/python3.6/runpy.py(85): _run_code\n/usr/lib/python3.6/runpy.py(193): _run_module_as_main\n\n\nGraph we tried to export:\ngraph(%input.1 : Float(1:3, 3:1, 512:3072, 1024:3, requires_grad=0, device=cuda:0),\n      %finalConv.weight : Float(19:48, 48:1, 1:1, 1:1, requires_grad=1, device=cuda:0),\n      %finalConv.bias : Float(19:1, requires_grad=1, device=cuda:0),\n      %735 : Float(16:27, 3:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %736 : Float(16:1, requires_grad=0, device=cuda:0),\n      %738 : Float(24:144, 16:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %739 : Float(24:1, requires_grad=0, device=cuda:0),\n      %741 : Float(32:216, 24:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %742 : Float(32:1, requires_grad=0, device=cuda:0),\n      %744 : Float(48:288, 32:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %745 : Float(48:1, requires_grad=0, device=cuda:0),\n      %747 : Float(10:432, 48:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %748 : Float(10:1, requires_grad=0, device=cuda:0),\n      %750 : Float(18:522, 58:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %751 : Float(18:1, requires_grad=0, device=cuda:0),\n      %753 : Float(10:162, 18:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %754 : Float(10:1, requires_grad=0, device=cuda:0),\n      %756 : Float(28:684, 76:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %757 : Float(28:1, requires_grad=0, device=cuda:0),\n      %759 : Float(64:48, 48:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %760 : Float(64:1, requires_grad=0, device=cuda:0),\n      %762 : Float(16:576, 64:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %763 : Float(16:1, requires_grad=0, device=cuda:0),\n      %765 : Float(28:720, 80:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %766 : Float(28:1, requires_grad=0, device=cuda:0),\n      %768 : Float(16:252, 28:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %769 : Float(16:1, requires_grad=0, device=cuda:0),\n      %771 : Float(46:972, 108:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %772 : Float(46:1, requires_grad=0, device=cuda:0),\n      %774 : Float(96:78, 78:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %775 : Float(96:1, requires_grad=0, device=cuda:0),\n      %777 : Float(18:864, 96:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %778 : Float(18:1, requires_grad=0, device=cuda:0),\n      %780 : Float(30:1026, 114:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %781 : Float(30:1, requires_grad=0, device=cuda:0),\n      %783 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %784 : Float(18:1, requires_grad=0, device=cuda:0),\n      %786 : Float(52:1296, 144:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %787 : Float(52:1, requires_grad=0, device=cuda:0),\n      %789 : Float(18:468, 52:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %790 : Float(18:1, requires_grad=0, device=cuda:0),\n      %792 : Float(30:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %793 : Float(30:1, requires_grad=0, device=cuda:0),\n      %795 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %796 : Float(18:1, requires_grad=0, device=cuda:0),\n      %798 : Float(88:1764, 196:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %799 : Float(88:1, requires_grad=0, device=cuda:0),\n      %801 : Float(160:160, 160:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %802 : Float(160:1, requires_grad=0, device=cuda:0),\n      %804 : Float(24:1440, 160:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %805 : Float(24:1, requires_grad=0, device=cuda:0),\n      %807 : Float(40:1656, 184:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %808 : Float(40:1, requires_grad=0, device=cuda:0),\n      %810 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %811 : Float(24:1, requires_grad=0, device=cuda:0),\n      %813 : Float(70:2016, 224:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %814 : Float(70:1, requires_grad=0, device=cuda:0),\n      %816 : Float(24:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %817 : Float(24:1, requires_grad=0, device=cuda:0),\n      %819 : Float(40:846, 94:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %820 : Float(40:1, requires_grad=0, device=cuda:0),\n      %822 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %823 : Float(24:1, requires_grad=0, device=cuda:0),\n      %825 : Float(118:2646, 294:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %826 : Float(118:1, requires_grad=0, device=cuda:0),\n      %828 : Float(224:214, 214:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %829 : Float(224:1, requires_grad=0, device=cuda:0),\n      %831 : Float(32:2016, 224:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %832 : Float(32:1, requires_grad=0, device=cuda:0),\n      %834 : Float(54:2304, 256:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %835 : Float(54:1, requires_grad=0, device=cuda:0),\n      %837 : Float(32:486, 54:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %838 : Float(32:1, requires_grad=0, device=cuda:0),\n      %840 : Float(92:2790, 310:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %841 : Float(92:1, requires_grad=0, device=cuda:0),\n      %843 : Float(32:828, 92:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %844 : Float(32:1, requires_grad=0, device=cuda:0),\n      %846 : Float(54:1116, 124:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %847 : Float(54:1, requires_grad=0, device=cuda:0),\n      %849 : Float(32:486, 54:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %850 : Float(32:1, requires_grad=0, device=cuda:0),\n      %852 : Float(158:3618, 402:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %853 : Float(158:1, requires_grad=0, device=cuda:0),\n      %855 : Float(320:286, 286:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %856 : Float(320:1, requires_grad=0, device=cuda:0),\n      %858 : Float(267:534, 534:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %859 : Float(267:1, requires_grad=0, device=cuda:0),\n      %861 : Float(24:2403, 267:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %862 : Float(24:1, requires_grad=0, device=cuda:0),\n      %864 : Float(40:2619, 291:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %865 : Float(40:1, requires_grad=0, device=cuda:0),\n      %867 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %868 : Float(24:1, requires_grad=0, device=cuda:0),\n      %870 : Float(70:2979, 331:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %871 : Float(70:1, requires_grad=0, device=cuda:0),\n      %873 : Float(24:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %874 : Float(24:1, requires_grad=0, device=cuda:0),\n      %876 : Float(40:846, 94:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %877 : Float(40:1, requires_grad=0, device=cuda:0),\n      %879 : Float(24:360, 40:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %880 : Float(24:1, requires_grad=0, device=cuda:0),\n      %882 : Float(118:3609, 401:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %883 : Float(118:1, requires_grad=0, device=cuda:0),\n      %885 : Float(187:374, 374:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %886 : Float(187:1, requires_grad=0, device=cuda:0),\n      %888 : Float(18:1683, 187:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %889 : Float(18:1, requires_grad=0, device=cuda:0),\n      %891 : Float(30:1845, 205:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %892 : Float(30:1, requires_grad=0, device=cuda:0),\n      %894 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %895 : Float(18:1, requires_grad=0, device=cuda:0),\n      %897 : Float(52:2115, 235:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %898 : Float(52:1, requires_grad=0, device=cuda:0),\n      %900 : Float(18:468, 52:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %901 : Float(18:1, requires_grad=0, device=cuda:0),\n      %903 : Float(30:630, 70:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %904 : Float(30:1, requires_grad=0, device=cuda:0),\n      %906 : Float(18:270, 30:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %907 : Float(18:1, requires_grad=0, device=cuda:0),\n      %909 : Float(88:2583, 287:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %910 : Float(88:1, requires_grad=0, device=cuda:0),\n      %912 : Float(119:238, 238:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %913 : Float(119:1, requires_grad=0, device=cuda:0),\n      %915 : Float(16:1071, 119:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %916 : Float(16:1, requires_grad=0, device=cuda:0),\n      %918 : Float(28:1215, 135:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %919 : Float(28:1, requires_grad=0, device=cuda:0),\n      %921 : Float(16:252, 28:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %922 : Float(16:1, requires_grad=0, device=cuda:0),\n      %924 : Float(46:1467, 163:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %925 : Float(46:1, requires_grad=0, device=cuda:0),\n      %927 : Float(63:126, 126:1, 1:1, 1:1, requires_grad=0, device=cuda:0),\n      %928 : Float(63:1, requires_grad=0, device=cuda:0),\n      %930 : Float(10:567, 63:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %931 : Float(10:1, requires_grad=0, device=cuda:0),\n      %933 : Float(18:657, 73:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %934 : Float(18:1, requires_grad=0, device=cuda:0),\n      %936 : Float(10:162, 18:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %937 : Float(10:1, requires_grad=0, device=cuda:0),\n      %939 : Float(28:819, 91:9, 3:3, 3:1, requires_grad=0, device=cuda:0),\n      %940 : Float(28:1, requires_grad=0, device=cuda:0)):\n  %417 : Tensor = onnx::Shape(%input.1)\n  %418 : Tensor = onnx::Constant[value={2}]()\n  %419 : Long(device=cpu) = onnx::Gather[axis=0](%417, %418) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:337:0\n  %420 : Tensor = onnx::Shape(%input.1)\n  %421 : Tensor = onnx::Constant[value={3}]()\n  %422 : Long(device=cpu) = onnx::Gather[axis=0](%420, %421) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:337:0\n  %734 : Float(1:2097152, 16:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%input.1, %735, %736)\n  %425 : Float(1:2097152, 16:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Relu(%734) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %737 : Float(1:3145728, 24:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%425, %738, %739)\n  %428 : Float(1:3145728, 24:131072, 256:512, 512:1, requires_grad=1, device=cuda:0) = onnx::Relu(%737) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %740 : Float(1:1048576, 32:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%428, %741, %742)\n  %431 : Float(1:1048576, 32:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%740) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %743 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%431, %744, %745)\n  %434 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%743) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %746 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%434, %747, %748)\n  %437 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%746) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %438 : Float(1:1900544, 58:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%437, %434) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %749 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%438, %750, %751)\n  %441 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%749) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %752 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%441, %753, %754)\n  %444 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%752) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %445 : Float(1:2490368, 76:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%444, %441, %434) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %755 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%445, %756, %757)\n  %448 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%755) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %449 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%437, %444, %448) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %758 : Float(1:2097152, 64:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%449, %759, %760)\n  %452 : Float(1:2097152, 64:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%758) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %453 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%452)\n  %454 : Float(1:524288, 64:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%453) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n  %761 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%454, %762, %763)\n  %457 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%761) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %458 : Float(1:655360, 80:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%457, %454) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %764 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%458, %765, %766)\n  %461 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%764) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %767 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%461, %768, %769)\n  %464 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%767) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %465 : Float(1:884736, 108:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%464, %461, %454) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %770 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%465, %771, %772)\n  %468 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%770) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %469 : Float(1:638976, 78:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%457, %464, %468) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %773 : Float(1:786432, 96:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%469, %774, %775)\n  %472 : Float(1:786432, 96:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%773) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %473 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%472)\n  %474 : Float(1:196608, 96:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%473) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n  %776 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%474, %777, %778)\n  %477 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%776) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %478 : Float(1:233472, 114:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%477, %474) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %779 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%478, %780, %781)\n  %481 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%779) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %782 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%481, %783, %784)\n  %484 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%782) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %485 : Float(1:294912, 144:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%484, %481, %474) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %785 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%485, %786, %787)\n  %488 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%785) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %788 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%488, %789, %790)\n  %491 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%788) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %492 : Float(1:143360, 70:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%491, %488) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %791 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%492, %792, %793)\n  %495 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%791) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %794 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%495, %795, %796)\n  %498 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%794) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %499 : Float(1:401408, 196:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%498, %495, %488, %474) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %797 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%499, %798, %799)\n  %502 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%797) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %503 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%477, %484, %491, %498, %502) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %800 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%503, %801, %802)\n  %506 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%800) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %507 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%506)\n  %508 : Float(1:81920, 160:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%507) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n  %803 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%508, %804, %805)\n  %511 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%803) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %512 : Float(1:94208, 184:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%511, %508) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %806 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%512, %807, %808)\n  %515 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%806) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %809 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%515, %810, %811)\n  %518 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%809) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %519 : Float(1:114688, 224:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%518, %515, %508) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %812 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%519, %813, %814)\n  %522 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%812) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %815 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%522, %816, %817)\n  %525 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%815) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %526 : Float(1:48128, 94:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%525, %522) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %818 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%526, %819, %820)\n  %529 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%818) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %821 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%529, %822, %823)\n  %532 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%821) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %533 : Float(1:150528, 294:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%532, %529, %522, %508) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %824 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%533, %825, %826)\n  %536 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%824) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %537 : Float(1:109568, 214:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%511, %518, %525, %532, %536) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %827 : Float(1:114688, 224:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%537, %828, %829)\n  %540 : Float(1:114688, 224:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%827) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %541 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%540)\n  %542 : Float(1:28672, 224:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%541) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/pooling.py:595:0\n  %830 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%542, %831, %832)\n  %545 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%830) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %546 : Float(1:32768, 256:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%545, %542) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %833 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%546, %834, %835)\n  %549 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%833) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %836 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%549, %837, %838)\n  %552 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%836) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %553 : Float(1:39680, 310:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%552, %549, %542) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %839 : Float(1:11776, 92:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%553, %840, %841)\n  %556 : Float(1:11776, 92:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%839) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %842 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%556, %843, %844)\n  %559 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%842) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %560 : Float(1:15872, 124:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%559, %556) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %845 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%560, %846, %847)\n  %563 : Float(1:6912, 54:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%845) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %848 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%563, %849, %850)\n  %566 : Float(1:4096, 32:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%848) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %567 : Float(1:51456, 402:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%566, %563, %556, %542) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %851 : Float(1:20224, 158:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%567, %852, %853)\n  %570 : Float(1:20224, 158:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%851) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %571 : Float(1:36608, 286:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%545, %552, %559, %566, %570) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %854 : Float(1:40960, 320:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%571, %855, %856)\n  %574 : Float(1:40960, 320:128, 8:16, 16:1, requires_grad=1, device=cuda:0) = onnx::Relu(%854) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %575 : Tensor = onnx::Shape(%537)\n  %576 : Tensor = onnx::Constant[value={2}]()\n  %577 : Long(device=cpu) = onnx::Gather[axis=0](%575, %576) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %578 : Tensor = onnx::Shape(%537)\n  %579 : Tensor = onnx::Constant[value={3}]()\n  %580 : Long(device=cpu) = onnx::Gather[axis=0](%578, %579) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %581 : Tensor = onnx::Unsqueeze[axes=[0]](%577)\n  %582 : Tensor = onnx::Unsqueeze[axes=[0]](%580)\n  %583 : Tensor = onnx::Concat[axis=0](%581, %582)\n  %584 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %585 : None = prim::Constant()\n  %586 : Float(1:163840, 320:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%574, %583, %584, %585) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n  %587 : Float(1:273408, 534:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%586, %537) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n  %857 : Float(1:136704, 267:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%587, %858, %859)\n  %590 : Float(1:136704, 267:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%857) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %860 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%590, %861, %862)\n  %593 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%860) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %594 : Float(1:148992, 291:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%593, %590) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %863 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%594, %864, %865)\n  %597 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%863) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %866 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%597, %867, %868)\n  %600 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%866) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %601 : Float(1:169472, 331:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%600, %597, %590) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %869 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%601, %870, %871)\n  %604 : Float(1:35840, 70:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%869) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %872 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%604, %873, %874)\n  %607 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%872) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %608 : Float(1:48128, 94:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%607, %604) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %875 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%608, %876, %877)\n  %611 : Float(1:20480, 40:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%875) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %878 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%611, %879, %880)\n  %614 : Float(1:12288, 24:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%878) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %615 : Float(1:205312, 401:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%614, %611, %604, %590) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %881 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%615, %882, %883)\n  %618 : Float(1:60416, 118:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Relu(%881) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %619 : Float(1:109568, 214:512, 16:32, 32:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%593, %600, %607, %614, %618) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %620 : Tensor = onnx::Shape(%503)\n  %621 : Tensor = onnx::Constant[value={2}]()\n  %622 : Long(device=cpu) = onnx::Gather[axis=0](%620, %621) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %623 : Tensor = onnx::Shape(%503)\n  %624 : Tensor = onnx::Constant[value={3}]()\n  %625 : Long(device=cpu) = onnx::Gather[axis=0](%623, %624) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %626 : Tensor = onnx::Unsqueeze[axes=[0]](%622)\n  %627 : Tensor = onnx::Unsqueeze[axes=[0]](%625)\n  %628 : Tensor = onnx::Concat[axis=0](%626, %627)\n  %629 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %630 : None = prim::Constant()\n  %631 : Float(1:438272, 214:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%619, %628, %629, %630) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n  %632 : Float(1:765952, 374:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%631, %503) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n  %884 : Float(1:382976, 187:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%632, %885, %886)\n  %635 : Float(1:382976, 187:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%884) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %887 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%635, %888, %889)\n  %638 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%887) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %639 : Float(1:419840, 205:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%638, %635) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %890 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%639, %891, %892)\n  %642 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%890) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %893 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%642, %894, %895)\n  %645 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%893) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %646 : Float(1:481280, 235:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%645, %642, %635) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %896 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%646, %897, %898)\n  %649 : Float(1:106496, 52:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%896) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %899 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%649, %900, %901)\n  %652 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%899) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %653 : Float(1:143360, 70:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%652, %649) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %902 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%653, %903, %904)\n  %656 : Float(1:61440, 30:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%902) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %905 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%656, %906, %907)\n  %659 : Float(1:36864, 18:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%905) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %660 : Float(1:587776, 287:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%659, %656, %649, %635) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %908 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%660, %909, %910)\n  %663 : Float(1:180224, 88:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Relu(%908) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %664 : Float(1:327680, 160:2048, 32:64, 64:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%638, %645, %652, %659, %663) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %665 : Tensor = onnx::Shape(%469)\n  %666 : Tensor = onnx::Constant[value={2}]()\n  %667 : Long(device=cpu) = onnx::Gather[axis=0](%665, %666) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %668 : Tensor = onnx::Shape(%469)\n  %669 : Tensor = onnx::Constant[value={3}]()\n  %670 : Long(device=cpu) = onnx::Gather[axis=0](%668, %669) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %671 : Tensor = onnx::Unsqueeze[axes=[0]](%667)\n  %672 : Tensor = onnx::Unsqueeze[axes=[0]](%670)\n  %673 : Tensor = onnx::Concat[axis=0](%671, %672)\n  %674 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %675 : None = prim::Constant()\n  %676 : Float(1:1310720, 160:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%664, %673, %674, %675) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n  %677 : Float(1:1949696, 238:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%676, %469) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n  %911 : Float(1:974848, 119:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%677, %912, %913)\n  %680 : Float(1:974848, 119:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%911) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %914 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%680, %915, %916)\n  %683 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%914) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %684 : Float(1:1105920, 135:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%683, %680) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %917 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%684, %918, %919)\n  %687 : Float(1:229376, 28:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%917) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %920 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%687, %921, %922)\n  %690 : Float(1:131072, 16:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%920) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %691 : Float(1:1335296, 163:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%690, %687, %680) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %923 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%691, %924, %925)\n  %694 : Float(1:376832, 46:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Relu(%923) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %695 : Float(1:638976, 78:8192, 64:128, 128:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%683, %690, %694) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %696 : Tensor = onnx::Shape(%449)\n  %697 : Tensor = onnx::Constant[value={2}]()\n  %698 : Long(device=cpu) = onnx::Gather[axis=0](%696, %697) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %699 : Tensor = onnx::Shape(%449)\n  %700 : Tensor = onnx::Constant[value={3}]()\n  %701 : Long(device=cpu) = onnx::Gather[axis=0](%699, %700) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:245:0\n  %702 : Tensor = onnx::Unsqueeze[axes=[0]](%698)\n  %703 : Tensor = onnx::Unsqueeze[axes=[0]](%701)\n  %704 : Tensor = onnx::Concat[axis=0](%702, %703)\n  %705 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %706 : None = prim::Constant()\n  %707 : Float(1:2555904, 78:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%695, %704, %705, %706) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n  %708 : Float(1:4128768, 126:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%707, %449) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:250:0\n  %926 : Float(1:2064384, 63:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%708, %927, %928)\n  %711 : Float(1:2064384, 63:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%926) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %929 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%711, %930, %931)\n  %714 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%929) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %715 : Float(1:2392064, 73:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%714, %711) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %932 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%715, %933, %934)\n  %718 : Float(1:589824, 18:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%932) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %935 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%718, %936, %937)\n  %721 : Float(1:327680, 10:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%935) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %722 : Float(1:2981888, 91:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%721, %718, %711) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:221:0\n  %938 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%722, %939, %940)\n  %725 : Float(1:917504, 28:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Relu(%938) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:1134:0\n  %726 : Float(1:1572864, 48:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%714, %721, %725) # /home/dongmin/FCHarDNet_Nextchip/ptsemseg/models/hardnet.py:232:0\n  %727 : Float(1:622592, 19:32768, 128:256, 256:1, requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%726, %finalConv.weight, %finalConv.bias) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/modules/conv.py:420:0\n  %728 : Tensor = onnx::Unsqueeze[axes=[0]](%419)\n  %729 : Tensor = onnx::Unsqueeze[axes=[0]](%422)\n  %730 : Tensor = onnx::Concat[axis=0](%728, %729)\n  %731 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %732 : None = prim::Constant()\n  %733 : Float(1:9961472, 19:524288, 512:1024, 1024:1, requires_grad=1, device=cuda:0) = aten::upsample_bilinear2d(%727, %730, %731, %732) # /home/dongmin/mindung/mindungmindung/lib/python3.6/site-packages/torch/nn/functional.py:3151:0\n  return (%733)\n"
     ]
    }
   ],
   "source": [
    "# Export the trained model to ONNX\n",
    "\n",
    "torch.onnx.export(model,images,\"FCHarDNet.onnx\",opset_version=9,verbose = True)\n",
    "#torch.onnx.export(model,images,\"FCHarDNet1.onnx\",opset_version=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-9f89f0481fb2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-9f89f0481fb2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    gitclone https://github.com/qfgaohao/pytorch-ssd.git\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "oml = onnx.load('FCHarDNet.onnx')  # onnx model\n",
    "onnx.checker.check_model(oml)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import shutil\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "\n",
    "from torch.utils import data\n",
    "from ptsemseg.models import get_model\n",
    "from ptsemseg.utils import convert_state_dict\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def init_model(model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    n_classes = 19\n",
    "\n",
    "    # Setup Model\n",
    "    model = get_model({\"arch\": \"hardnet\"}, n_classes)\n",
    "    state = convert_state_dict(torch.load(model_path, map_location=device)[\"model_state\"])\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    return device, model\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "model_path = 'weights/hardnet70_cityscapes_model.pkl'\n",
    "device, model = init_model(model_path)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "dummy_input = np.random.rand(512,1024,3)  # uint8 with RGB mode (h,w,c)\n",
    "img = dummy_input.astype(np.float16)\n",
    "\n",
    "# norm\n",
    "value_scale = 255\n",
    "mean = [0.406, 0.456, 0.485]\n",
    "mean = [item * value_scale for item in mean]\n",
    "std = [0.225, 0.224, 0.229]\n",
    "std = [item * value_scale for item in std]\n",
    "img = (img - mean) / std\n",
    "\n",
    "# NHWC -> NCHW (n=batch size(한 배치 안의 이미지 개수),c,h,w)로 바꿔줌\n",
    "img = img.transpose(2, 0, 1)\n",
    "img = np.expand_dims(img, 0) #batch로 만들어줌\n",
    "img = torch.from_numpy(img).float()\n",
    "\n",
    "images = img.to(device) #gpu에 image올림\n",
    "outputs = model(images)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "torch.onnx.export(model,images,\"FCHarDNet.onnx\",opset_version=11,verbose = True)\n",
    "#torch.onnx.export(model,images,\"FCHarDNet1.onnx\",opset_version=9)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "#print(model)\n",
    "\n",
    "oml = onnx.load('FCHarDNet.onnx')  # onnx model\n",
    "onnx.checker.check_model(oml)\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cis true\n",
    "label_idx_0 = []\n",
    "for i in range(len(cis_test_y)):\n",
    "    if(cis_test_y[i]==0):\n",
    "        label_idx_0.append(i)\n",
    "        \n",
    "data_0 = np.zeros(shape = (len(label_idx_0),2))\n",
    "for i in range(len(label_idx_0)):\n",
    "    data_0[i] = cis_test_x[label_idx_0[i]]\n",
    "\n",
    "label_idx_1 = []\n",
    "for i in range(len(cis_test_y)):\n",
    "    if(cis_test_y[i]==1):\n",
    "        label_idx_1.append(i)\n",
    "        \n",
    "data_1 = np.zeros(shape = (len(label_idx_1),2))\n",
    "for i in range(len(label_idx_1)):\n",
    "    data_1[i] = cis_test_x[label_idx_1[i]]\n",
    "    \n",
    "\n",
    "plt.scatter(data_0[:,0], data_0[:,1], c = 'k', label='0')\n",
    "plt.scatter(data_1[:,0], data_1[:,1], c = 'w', label='1')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
